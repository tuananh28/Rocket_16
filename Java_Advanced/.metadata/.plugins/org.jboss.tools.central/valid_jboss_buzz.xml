<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Why should I choose Quarkus over Spring for my microservices?</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ma5T-Pjb6MM/why-should-i-choose-quarkus-over-spring-my-microservices" /><author><name>Eric Deandrea</name></author><id>f6484334-5e02-4fbb-bdd4-365927273e1c</id><updated>2021-08-31T14:00:00Z</updated><published>2021-08-31T14:00:00Z</published><summary type="html">&lt;p&gt;As interest grows in &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/enterprise-java/"&gt;Java&lt;/a&gt; developers have struggled to make applications smaller and faster to meet today’s demands and requirements. In the modern computing environment, applications must respond to requests quickly and efficiently, be suitable for running in volatile environments such as virtual machines or containers, and support rapid development. Because of this, Java, and popular Java runtimes, are sometimes considered inferior to runtimes in other languages such as &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; and &lt;a href="https://golang.org/"&gt;Go&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The Java language and platform have been very successful over the years, preserving Java as the predominant language in current use. &lt;a href="https://www.grandviewresearch.com/industry-analysis/application-server-market"&gt;Analysts have estimated the global application server market size&lt;/a&gt; at $15.84 billion in 2020, with expectations of growing at a rate of 13.2% from 2021 to 2028. Additionally, tens of millions of Java developers worldwide work for organizations that run their businesses using Java. Faced with today’s challenges, these organizations need to adapt and adopt new ways of building and deploying applications. Forgoing Java for other application stacks isn’t a choice for many organizations. It would involve re-training their development staff and re-implementing processes to release and monitor applications in production.&lt;/p&gt; &lt;h2&gt;Java is still relevant&lt;/h2&gt; &lt;p&gt;With additions to Java and Java frameworks over the past few years, Java can proudly retain its role as the primary language for enterprise applications. &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Quarkus&lt;/a&gt;, an open source project introduced by Red Hat, is one such framework that has taken the Java community by storm. Quarkus combines developer productivity and joy with the speed and performance of Go.&lt;/p&gt; &lt;p&gt;Quarkus integrates with other modern Java frameworks and libraries that many developers are already familiar with and most likely using. Among the specifications and technologies underlying and integrated with Quarkus are Eclipse MicroProfile, Eclipse Vert.x, Contexts and Dependency Injection (CDI), Jakarta RESTful Web Services (JAX-RS), the Java Persistence API (JPA), the Java Transaction API (JTA), Apache Camel, and Hibernate, just to name a few.&lt;/p&gt; &lt;p&gt;Quarkus and &lt;a href="https://developers.redhat.com/topics/spring-boot/"&gt;Spring&lt;/a&gt; address many of the same types of applications, but because it was born in today’s day and age, Quarkus has the advantage of starting with a clean slate. Quarkus can focus on innovation in modern areas of development pertaining to scalable, cloud-hosted applications because it doesn’t have to retrofit new patterns and principles into an existing codebase that has evolved over time.&lt;/p&gt; &lt;h2&gt;But I already know Spring ...&lt;/h2&gt; &lt;p&gt;&lt;a href="https://red.ht/quarkus-spring-devs"&gt;&lt;em&gt;Quarkus for Spring Developers&lt;/em&gt;&lt;/a&gt; introduces Quarkus to Java developers with a special eye to helping those familiar with Spring’s concepts, constructs, and conventions learn Quarkus quickly. Spring developers should immediately recognize and be able to apply patterns they are already familiar with, and in many instances, using the same underlying technologies. Using Kotlin in your Spring applications? Great—you can continue using Kotlin with Quarkus.&lt;/p&gt; &lt;p&gt;Chapters are devoted to getting started, RESTful applications, persistence, &lt;a href="https://developers.redhat.com/topics/event-driven/"&gt;event-driven&lt;/a&gt; services, and cloud environments such as containers and &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;. Each chapter offers like-for-like examples and emphasizes testing patterns and practices with such applications, while also differentiating Quarkus from Spring.&lt;/p&gt; &lt;p&gt;Additionally, Quarkus provides a set of &lt;a href="https://quarkus.io/guides/spring-di#more-spring-guides"&gt;extensions for various Spring APIs&lt;/a&gt;. These extensions help simplify the process of learning Quarkus or migrating existing Spring applications to Quarkus, capitalizing on a developer’s Spring knowledge to accelerate the learning curve to adopt Quarkus. In some cases, an &lt;a href="https://developers.redhat.com/blog/2021/02/09/spring-boot-on-quarkus-magic-or-madness"&gt;existing Spring application may even be able to run in Quarkus&lt;/a&gt; without any code changes.&lt;/p&gt; &lt;h2&gt;How can Quarkus help me?&lt;/h2&gt; &lt;p&gt;Quarkus has many features and capabilities that can help both developers and operations teams.&lt;/p&gt; &lt;h3&gt;Enhancing developer productivity&lt;/h3&gt; &lt;p&gt;Since its inception in early 2019, Quarkus has focused on more than just delivering features. Developer productivity and joy have been critical goals. With every new feature, Quarkus carefully considers the developer experience and how to improve it.&lt;/p&gt; &lt;p&gt;The development process is faster and more pleasant with Quarkus's live coding feature. Quarkus can automatically detect changes made to Java and other resource and configuration files, then transparently re-compile and re-deploy the changes. Usually, within a second, you can view your application’s output or compiler error messages. This feature can also be used with Quarkus applications running in a remote environment. The remote capability is useful where rapid development or prototyping is needed but provisioning services in a local environment isn’t feasible or possible.&lt;/p&gt; &lt;p&gt;Quarkus takes this concept a step further with its &lt;a href="https://youtu.be/0JiE-bRt-GU"&gt;continuous testing&lt;/a&gt; feature to facilitate test-driven development. As changes are made to the application source code, Quarkus can automatically rerun affected tests in the background, giving developers instant feedback about the code they are writing or modifying.&lt;/p&gt; &lt;p&gt;Need a database for your application? Kafka broker? Redis server? AMQP broker? OpenID Connect authentication server? API/Schema registry? Quarkus &lt;a href="https://quarkus.io/guides/datasource#dev-services-configuration-free-databases"&gt;Dev Services for databases&lt;/a&gt; (see the &lt;a href="https://youtu.be/szza3DZlKzA"&gt;video demo&lt;/a&gt;), &lt;a href="https://quarkus.io/guides/kafka-dev-services"&gt;Dev Services for Kafka&lt;/a&gt; (see the &lt;a href="https://youtu.be/z2ZceqVQ20E"&gt;video demo&lt;/a&gt;), &lt;a href="https://quarkus.io/guides/redis-dev-services"&gt;Dev Services for Redis&lt;/a&gt;, &lt;a href="https://quarkus.io/guides/amqp-dev-services"&gt;Dev Services for AMQP&lt;/a&gt;, &lt;a href="https://quarkus.io/guides/security-openid-connect-dev-services"&gt;Dev Services for OpenID Connect&lt;/a&gt; (see the &lt;a href="https://youtu.be/coG5ZbLgjJs"&gt;video demo&lt;/a&gt;), and &lt;a href="https://quarkus.io/guides/apicurio-registry-dev-services"&gt;Dev Services for Apicurio Registry&lt;/a&gt; have you covered. Dev Services makes development faster by providing needed infrastructure automatically, eliminating all the required provisioning and configuration hassle. New Dev Services are added with each new release.&lt;/p&gt; &lt;p&gt;Following the philosophy of simplicity and enhancing developer productivity, &lt;a href="https://quarkus.io/guides/building-native-image"&gt;building an application into a native image&lt;/a&gt; is extremely simple. All the heavy-lifting and integration to consume GraalVM is done for you by the Quarkus build tools. Developers or &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;CI/CD&lt;/a&gt; systems simply need to run a build, just like any other Java build, to produce a native executable. Tests can even be run against the built artifact.&lt;/p&gt; &lt;h3&gt;Kubernetes native&lt;/h3&gt; &lt;p&gt;From the beginning, Quarkus was designed around Kubernetes-native philosophies, optimizing for low memory usage and fast startup times. As much processing as possible is done at build time. Classes used only at application startup are invoked at build time and not loaded into the runtime JVM, reducing the size, and ultimately the memory footprint, of the application running on the JVM.&lt;/p&gt; &lt;p&gt;This design accounted for native compilation from the onset, enabling Quarkus to be "natively native." Similar native capabilities in Spring are still considered experimental or beta, and in some instances, not even available. Coupled with a runtime platform like Kubernetes, more Quarkus applications can be deployed within a given set of resources than other Java or Spring applications.&lt;/p&gt; &lt;h3&gt;To be or not to be reactive?&lt;/h3&gt; &lt;p&gt;With Spring, a developer needs to decide up front, before writing a line of code, which architecture to follow for an application. This choice determines the entire set of libraries that a developer uses in a Spring application. Quarkus does not have such limitations because it was born in the reactive era. Quarkus, at its core, is based on a fully reactive and non-blocking architecture powered by &lt;a href="https://vertx.io"&gt;Eclipse Vert.x&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Quarkus integrates deeply with Vert.x, allowing developers to utilize both blocking (imperative) and non-blocking (reactive) libraries and APIs. In most cases, developers can use both blocking and reactive APIs within the same classes. &lt;a href="https://quarkus.io/blog/resteasy-reactive-smart-dispatch/"&gt;Quarkus ensures&lt;/a&gt; that the blocking APIs will block appropriately while the reactive APIs remain non-blocking.&lt;/p&gt; &lt;h2&gt;Ready to give it a try?&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://red.ht/dev-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, which offers a free and ready-made environment for trying out containerized applications, is &lt;a href="https://developers.redhat.com/articles/2021/05/31/learn-quarkus-faster-quick-starts-developer-sandbox-red-hat-openshift"&gt;set up to support Quarkus&lt;/a&gt;. Whether you run on-premises, on &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Red Hat OpenShift&lt;/a&gt;, or in another cloud setting, the many open source capabilities of Quarkus are available.&lt;/p&gt; &lt;p&gt;&lt;a href="https://red.ht/quarkus-spring-devs"&gt;&lt;em&gt;Quarkus for Spring Developers&lt;/em&gt;&lt;/a&gt; will help get you started and guide you through your journey.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/31/why-should-i-choose-quarkus-over-spring-my-microservices" title="Why should I choose Quarkus over Spring for my microservices?"&gt;Why should I choose Quarkus over Spring for my microservices?&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ma5T-Pjb6MM" height="1" width="1" alt=""/&gt;</summary><dc:creator>Eric Deandrea</dc:creator><dc:date>2021-08-31T14:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/31/why-should-i-choose-quarkus-over-spring-my-microservices</feedburner:origLink></entry><entry><title>Game telemetry with Kafka Streams and Quarkus, Part 2</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/qjWAG2-BvTg/game-telemetry-kafka-streams-and-quarkus-part-2" /><author><name>Evan Shortiss</name></author><id>755954fe-2569-44ba-949c-fdd7393a6cc4</id><updated>2021-08-31T07:00:00Z</updated><published>2021-08-31T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://developers.redhat.com/articles/2021/08/24/game-telemetry-kafka-streams-and-quarkus-part-1"&gt;first half of this article&lt;/a&gt; introduced &lt;a href="https://arcade.redhat.com/shipwars"&gt;Shipwars&lt;/a&gt;, a browser-based video game that’s similar to the classic &lt;a href="https://en.wikipedia.org/wiki/Battleship_(game)"&gt;Battleship&lt;/a&gt; tabletop game, but with a server-side AI opponent. We set up a development environment to analyze real-time gaming data and I explained some of the ways you might use game data analysis and telemetry data to improve a product.&lt;/p&gt; &lt;p&gt;In this second half, we'll run the analytics and use the captured data to replay games.&lt;/p&gt; &lt;h2&gt;Using Kafka Streams for game analytics&lt;/h2&gt; &lt;p&gt;We're using the &lt;a href="https://kafka.apache.org/documentation/streams/"&gt;Kafka Streams API&lt;/a&gt; along with &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; and &lt;a href="https://www.openshift.com/products/application-services"&gt;Red Hat OpenShift Application Services&lt;/a&gt; in the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;. Please see the first half of this article to set up the environment for analyzing data captured during gameplay.&lt;/p&gt; &lt;p&gt;The Shipwars architecture contains several independent Java applications that use the Kafka Streams API. The source code for these applications is in the &lt;a href="https://github.com/evanshortiss/shipwars-streams"&gt;shipwars-streams repository&lt;/a&gt; on GitHub. We'll deploy each of the applications for game data analysis.&lt;/p&gt; &lt;h2&gt;Deploy the Kafka Streams enricher&lt;/h2&gt; &lt;p&gt;The first Kafka Streams application you’ll deploy is an &lt;em&gt;enricher&lt;/em&gt;. It performs a join between two sources of data: Players and Attacks (also known as "shots"). Specifically, this application performs a &lt;a href="https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html#kstream-globalktable-join"&gt;KStream-GlobalKTable join&lt;/a&gt;. The join involves deserializing the JSON data from two Kafka topics into POJOs using &lt;a href="https://kafka.apache.org/10/documentation/streams/developer-guide/datatypes"&gt;Serdes&lt;/a&gt; for serialization and deserialization. Then, we use the &lt;a href="https://kafka.apache.org/documentation/streams/developer-guide/"&gt;Kafka Streams DSL&lt;/a&gt; to join the data sources.&lt;/p&gt; &lt;p&gt;A &lt;code&gt;GlobalKTable&lt;/code&gt; stores key-value pairs. In this application, the key is the player ID and the value is the player data; that is, the username and whether they are a (supposed) human or an AI bot. An entry is added to this table each time a player is created by the game server, because the game server emits an event with the player data to the &lt;code&gt;shipwars-players&lt;/code&gt; topic that the &lt;code&gt;GlobalKTable&lt;/code&gt; is subscribed to.&lt;/p&gt; &lt;p&gt;A &lt;code&gt;KStream&lt;/code&gt; is an abstraction above a stream of events in a specific topic. Kafka Streams applications can map, filter, and even join &lt;code&gt;KStream&lt;/code&gt; instances with one another and instances of &lt;code&gt;GlobalKTable&lt;/code&gt;. The Shipwars application demonstrates this feature.&lt;/p&gt; &lt;p&gt;Every attack made in Shipwars arrives on a &lt;code&gt;KStream&lt;/code&gt; subscribed to the &lt;code&gt;shipwars-attacks&lt;/code&gt; topic. It is then joined with the associated player data in the &lt;code&gt;GlobalKTable&lt;/code&gt; to create a new record. The relevant code is included in Figure 1. Also see the &lt;a href="https://github.com/evanshortiss/shipwars-streams/blob/main/shot-stream-enricher/src/main/java/org/acme/kafka/streams/enricher/streams/TopologyShotMapper.java#L29-L82"&gt;a TopologyShotMapper repository&lt;/a&gt; on GitHub.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_1.png?itok=bkI7ybai" width="600" height="688" alt="The enricher is a Kafka Streams application that performs a KStream-GlobalKTable join and writes the join result to a new Kafka topic." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Kafka Streams DSL code for the enricher application. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Deploy the application&lt;/h3&gt; &lt;p&gt;Take the following steps to deploy the Kafka Streams enricher application:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Navigate to your project in the Developer Sandbox UI.&lt;/li&gt; &lt;li&gt;Ensure that the &lt;strong&gt;Developer&lt;/strong&gt; view is selected in the side menu.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;+Add&lt;/strong&gt; link and select &lt;strong&gt;Container Image&lt;/strong&gt; from the available options.&lt;/li&gt; &lt;li&gt;Paste &lt;code&gt;quay.io/evanshortiss/shipwars-streams-shot-enricher&lt;/code&gt; into the &lt;strong&gt;Image name field&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Select Quarkus as the &lt;strong&gt;Runtime icon&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Under &lt;strong&gt;General&lt;/strong&gt;, select &lt;strong&gt;Create Application&lt;/strong&gt; and enter &lt;code&gt;shipwars-analysis&lt;/code&gt; as the &lt;strong&gt;Application name&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Leave the &lt;strong&gt;Name&lt;/strong&gt; as the default value.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Deployment&lt;/strong&gt; link under the &lt;strong&gt;Advanced options&lt;/strong&gt;. This reveals a new form element to enter environment variables.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Add from ConfigMap or Secret&lt;/strong&gt; button and create the following variables (also shown in Figure 2): &lt;ol&gt;&lt;li&gt;&lt;code&gt;KAFKA_BOOTSTRAP_SERVERS&lt;/code&gt;: Select the generated &lt;code&gt;shipwars&lt;/code&gt; secret and use the &lt;code&gt;bootstrapServers&lt;/code&gt; key as the value.&lt;/li&gt; &lt;li&gt;&lt;code&gt;KAFKA_CLIENT_ID&lt;/code&gt;: Select the &lt;code&gt;rh-cloud-services-service-account&lt;/code&gt; secret and use the &lt;code&gt;client-id&lt;/code&gt; value.&lt;/li&gt; &lt;li&gt;&lt;code&gt;KAFKA_CLIENT_SECRET&lt;/code&gt;: Select the &lt;code&gt;rh-cloud-services-service-account&lt;/code&gt; secret and use the &lt;code&gt;client-secret&lt;/code&gt; value.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create&lt;/strong&gt; button to deploy the application.&lt;/li&gt; &lt;/ol&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_2.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_2.jpg?itok=dSf893Vt" width="600" height="366" alt="The Deployment screen in the OpenShift console can configure the Kafka Streams enricher application." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Configuring the Kafka Streams enricher application. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Play a round of Shipwars&lt;/h3&gt; &lt;p&gt;Now you’ve successfully deployed the first Kafka Streams application. It should appear as a &lt;a href="https://developers.redhat.com/products/quarkus"&gt;Quarkus&lt;/a&gt; icon in the OpenShift Topology view. The blue ring around it indicates that the application is in a healthy running state.&lt;/p&gt; &lt;p&gt;Next, open the &lt;code&gt;shipwars-client&lt;/code&gt; NGINX application URL, then open the Kafka Streams &lt;code&gt;shipwars-streams-shot-enricher&lt;/code&gt; application pod logs in another browser window. Play a match of Shipwars and watch the pod logs as you play. The &lt;code&gt;shipwars-streams-shot-enricher&lt;/code&gt; application prints a log each time it receives an attack event, and joins the event information with the associated player record. This joined record is then written to the &lt;code&gt;shipwars-attacks-lite&lt;/code&gt; Kafka topic.&lt;/p&gt; &lt;p&gt;If you’d like to view the data in the new topic, you can do so using a Kafka client such as &lt;a href="https://github.com/edenhill/kafkacat"&gt;kafkacat&lt;/a&gt;, which is shown in Figure 3.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_3.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_3.jpg?itok=_CETulbe" width="600" height="366" alt="The kafkacat CLI can show enriched shots. All the shots displayed in this screenshot are from the same match between an AI player and a human player. " typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Using the kafkacat CLI to view enriched shots between an AI and a human player. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Deploy the Kafka Streams aggregators&lt;/h2&gt; &lt;p&gt;The next two Kafka Streams applications perform aggregations. These aggregations are stateful operations that track the rolling state of a value for a particular key.&lt;/p&gt; &lt;h3&gt;Shot distribution&lt;/h3&gt; &lt;p&gt;The first application, the shot distribution aggregator, aggregates the total number of shots against each cell on the 5x5 grid used by Shipwars. This aggregate breaks down into the following integer values:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;AI hit&lt;/li&gt; &lt;li&gt;AI miss&lt;/li&gt; &lt;li&gt;Human hit&lt;/li&gt; &lt;li&gt;Human miss&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To create this aggregation, you need the data output by the Kafka Streams enricher you deployed previously. The data in the &lt;code&gt;shipwars-attacks&lt;/code&gt; topic doesn’t specify whether the attacker was a human or AI. The joined data does specify this important piece of information.&lt;/p&gt; &lt;p&gt;There are two other interesting aspects of this application:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;It uses &lt;a href="https://github.com/eclipse/microprofile-reactive-streams-operators]"&gt;MicroProfile Reactive Streams &lt;/a&gt;to expose the enriched data stream via HTTP server-sent events. You’ll see this in action soon.&lt;/li&gt; &lt;li&gt;It stores the aggregated data and exposes it for queries via HTTP REST endpoints.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;You will find the Kafka Streams DSL code for the shot distribution aggregator in the &lt;a href="https://github.com/evanshortiss/shipwars-streams/blob/main/shot-distribution-aggregator/src/main/java/org/acme/kafka/streams/aggregator/streams/TopologyShotAnalysis.java"&gt;TopologyShotAnalysis repository&lt;/a&gt;. Figure 4 shows the relevant code.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_4.png?itok=plS7ahz2" width="600" height="539" alt="Aggregation using the Kafka Streams DSL. This code consumes the enriched attacks or shots, and creates an aggregation that contains counts of hits and misses. The aggregation is keyed for each deployment of the game server. New deployments of the game server result in new aggregate records." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Aggregation using the Kafka Streams DSL. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;We used the second aggregation at Red Hat Summit 2021. It forms a complete record of all turns for each match, so that you can analyze them later or even replay them as we did in our demonstration.&lt;/p&gt; &lt;h3&gt;Deploy the aggregators&lt;/h3&gt; &lt;p&gt;Both applications can be deployed in the exact same manner as the enricher. Simply use &lt;code&gt;quay.io/evanshortiss/shipwars-streams-shot-distribution&lt;/code&gt; and &lt;code&gt;quay.io/evanshortiss/shipwars-streams-match-aggregates&lt;/code&gt; as the &lt;strong&gt;Image name&lt;/strong&gt; and add each to the &lt;code&gt;shipwars-analysis&lt;/code&gt; application that you created for the enricher application. View the logs once each application has started. Assuming you played a Shipwars match, you should immediately see logs stating that the aggregate shots record has been updated for a specific game ID.&lt;/p&gt; &lt;p&gt;Use the &lt;strong&gt;Open URL&lt;/strong&gt; button on the &lt;code&gt;shipwars-streams-shot-distribution&lt;/code&gt; node in the OpenShift Topology view to access the exposed route for the application. Append &lt;code&gt;/shot-distribution&lt;/code&gt; to the opened URL, in the following format:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;http://shipwars-streams-shot-distribution-$USERNAME-dev.$SUBDOMAIN.openshiftapps.com/shot-distribution&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The endpoint will return the aggregations in JSON format, as shown in Figure 5. The top-level keys are the game generation IDs. Each nested key represents a cell (X, Y coordinate) on the game grid, and contains the resulting hits and misses against that cell, classified by player type.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_5.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_5.jpg?itok=uYqD59WW" width="600" height="366" alt="An aggregation of shots for a given game server generation/deployment is shown in a JSON array, divided into hits and misses for the AI and the human player." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Aggregation of shots for a given game server generation. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Using the aggregated data&lt;/h3&gt; &lt;p&gt;This aggregated data could be used in various ways. The &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; AI move service could query it and adapt the game difficulty based on the hit rate of players. Alternatively, we could use the data to display a heatmap of shot activity, as you’ll see shortly.&lt;/p&gt; &lt;p&gt;You can view aggregated match records using the same approach. Click the &lt;strong&gt;Open URL&lt;/strong&gt; button on the &lt;code&gt;shipwars-streams-match-aggregate&lt;/code&gt; node in the OpenShift Topology view, and append &lt;code&gt;/replays&lt;/code&gt; to the URL to view match records.&lt;/p&gt; &lt;h2&gt;Analyzing attacks in real-time&lt;/h2&gt; &lt;p&gt;We use SmallRye and MicroProfile Reactive Streams, exposed by the &lt;code&gt;shipwars-streams-shot-distribution&lt;/code&gt; application, to create visualizations of the attacks on a real-time heatmap. The application is custom in this case, but it’s common to ingest the data into other analysis tools and systems using Kafka Connect.&lt;/p&gt; &lt;p&gt;You can test this endpoint by starting a cURL request using the following commands, and then playing the Shipwars game in your browser:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export AGGREGATOR_ROUTE=$(oc get route shipwars-streams-shot-distribution -o jsonpath='{.spec.host}') $ curl http://$AGGREGATOR_ROUTE/shot-distribution/stream&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Your cURL request should print data similar to the result shown in Figure 6.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_6.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_6.jpg?itok=--uxaAa-" width="600" height="366" alt="A cURL command displays a stream of server-sent events." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Using cURL to display a stream of server-sent events. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Deploy the heatmap application&lt;/h3&gt; &lt;p&gt;Once you’re satisfied that the endpoint is working as expected, you can deploy the heatmap application. This is a web-based application that uses TypeScript, Tailwind CSS, and Parcel Bundler. We're using &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; to run the build tools, and NGINX serves the resulting HTML, CSS, and JavaScript.&lt;/p&gt; &lt;p&gt;Use the &lt;code&gt;oc new-app&lt;/code&gt; command to build the application on the Developer Sandbox via a &lt;a href="https://github.com/openshift/source-to-image"&gt;source-to-image&lt;/a&gt; process, and deploy the application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# The builder/tools image export BUILDER=quay.io/evanshortiss/s2i-nodejs-nginx # Source code to build export SOURCE=https://github.com/evanshortiss/shipwars-visualisations # The public endpoint for the API exposing stream data export ROUTE=$(oc get route shipwars-streams-shot-distribution -o jsonpath='{.spec.host}') oc new-app $BUILDER~$SOURCE \ --name shipwars-visualisations \ --build-env STREAMS_API_URL=http://$ROUTE/ \ -l app.kubernetes.io/part-of=shipwars-analysis \ -l app.openshift.io/runtime=nginx&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The new application shows up in the OpenShift Topology view immediately, but you’ll need to wait for the build to complete before the application becomes usable. Click the &lt;strong&gt;Open URL&lt;/strong&gt; button to view the application UI when the build has finished. A loading spinner is displayed initially, but once you start to play Shipwars in another browser window the heatmap will update in real-time, similar to the display in Figure 7.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_7.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_7.jpg?itok=jueBAeNw" width="600" height="366" alt="A heatmap, updated in real-time, shows how much each square has been attacked in Shipwars. Dark squares have been targeted by more attacks than light squares." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: A real-time heatmap shows how much each square has been attacked in Shipwars. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Squares that are targeted frequently will appear darker than squares that are targeted less often. Of course, our game has a low resolution of only 5x5, but in a game with larger maps (such as the first-person shooter games mentioned in the first half of this article) players would tend to spend more time in areas with better weapons and tactical cover. This would result in a useful heatmap for analyzing player behavior.&lt;/p&gt; &lt;h2&gt;Viewing the replays&lt;/h2&gt; &lt;p&gt;The interface we used to view replays at Red Hat Summit 2021 was created by Luke Dary. You can view the aggregated match replays by deploying the UI like so:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-app quay.io/evanshortiss/shipwars-replay-ui \ -l "app.kubernetes.io/part-of=shipwars-analysis" \ -e REPLAY_SERVER="http://shipwars-streams-match-aggregates:8080" \ --name shipwars-replay $ oc expose svc shipwars-replay&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the application is deployed, you can view the replays by opening the application URL in a web browser. The replay UI is shown in Figure 8.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_8.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_8.png?itok=wG7F4Nhp" width="600" height="514" alt="Replays of the Shipwars match are aggregated by Kafka Streams." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: The Shipwars match replays, aggregated by Kafka Streams. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;If you made it this far, well done! You’ve learned how to:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Use the Red Hat OpenShift Application Services CLI to interact with OpenShift Streams for Apache Kafka.&lt;/li&gt; &lt;li&gt;Connect your OpenShift environment to your managed Kafka instances.&lt;/li&gt; &lt;li&gt;Deploy applications into an OpenShift environment using the OpenShift UI and CLI.&lt;/li&gt; &lt;li&gt;Use Kafka Streams to create data processing architectures with OpenShift Streams for Apache Kafka.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Using an OpenShift cluster and OpenShift Streams for Apache Kafka allows you to focus on building applications instead of infrastructure. Better yet, your applications can run anywhere and still utilize OpenShift Streams for Apache Kafka. The Shipwars application now includes &lt;a href="https://github.com/redhat-gamedev/shipwars-deployment#docker-compose"&gt;instructions for a Docker Compose&lt;/a&gt; deployment that can connect to the managed Kafka service. We suggest giving that a try next.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/31/game-telemetry-kafka-streams-and-quarkus-part-2" title="Game telemetry with Kafka Streams and Quarkus, Part 2"&gt;Game telemetry with Kafka Streams and Quarkus, Part 2&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/qjWAG2-BvTg" height="1" width="1" alt=""/&gt;</summary><dc:creator>Evan Shortiss</dc:creator><dc:date>2021-08-31T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/31/game-telemetry-kafka-streams-and-quarkus-part-2</feedburner:origLink></entry><entry><title type="html">Quarkus 2.2.1.Final released - Hardening release</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/uMugHqAvPEo/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-2-2-1-final-released/</id><updated>2021-08-31T00:00:00Z</updated><content type="html">Today we announce the availability of Quarkus 2.2.1.Final, which is the result of our first hardening cycle. Indeed, for 2.2, we decided to slow down on adding new features and focus this release cycle on hardening Quarkus with 3 main focuses: Fix issues Improve usability Improve documentation Thus the list...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/uMugHqAvPEo" height="1" width="1" alt=""/&gt;</content><dc:creator>Guillaume Smet</dc:creator><feedburner:origLink>https://quarkus.io/blog/quarkus-2-2-1-final-released/</feedburner:origLink></entry><entry><title>Automate Red Hat JBoss Web Server deployments with Ansible</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/lYvQvwWxVTc/automate-red-hat-jboss-web-server-deployments-ansible" /><author><name>Romain Pelisse</name></author><id>bb17fe33-34bd-4ea4-ae15-f2317bd13289</id><updated>2021-08-30T07:00:00Z</updated><published>2021-08-30T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/webserver/overview"&gt;Red Hat JBoss Web Server&lt;/a&gt; combines a web server (Apache HTTPD), a servlet engine (Apache Tomcat), and modules for load balancing (&lt;code&gt;mod_jk&lt;/code&gt; and &lt;code&gt;mod_cluster&lt;/code&gt;). Ansible is one of the best automation tools on the market. In this article, we'll use Ansible to completely automate the deployment of a JBoss Web Server instance on a freshly installed Red Hat Enterprise Linux (RHEL) server.&lt;/p&gt; &lt;p&gt;Our objective is to automate a JBoss Web Server deployment through the following tasks:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Retrieve the archive containing JBoss Web Server from a repository and install the files it contains on the system.&lt;/li&gt; &lt;li&gt;Configure the Red Hat Enterprise Linux operating system (create users, groups, and the required setup files to make JBoss Web Server a systemd service).&lt;/li&gt; &lt;li&gt;Fine-tune the configuration of the JBoss Web Server server itself (bind it to the appropriate interface and port).&lt;/li&gt; &lt;li&gt;Deploy a web application and starting the systemd service.&lt;/li&gt; &lt;li&gt;Perform a health check if the deployed application is accessible.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Ansible fully automates these operations, with no manual steps required.&lt;/p&gt; &lt;h2&gt;Set the target environment&lt;/h2&gt; &lt;p&gt;Before we start the automation, we need to specify our target environment. In this case, we're using RHEL 8 with Python 3.6.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# cat /etc/redhat-release Red Hat Enterprise Linux release 8.4 (Ootpa) # ansible --version ansible 2.9.22   config file = /etc/ansible/ansible.cfg   configured module search path = ['/root/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']   ansible python module location = /usr/lib/python3.6/site-packages/ansible   executable location = /usr/bin/ansible   python version = 3.6.8 (default, Mar 18 2021, 08:58:41) [GCC 8.4.1 20200928 (Red Hat 8.4.1-1)]&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The playbook might not work if you want to use a different Python version or target operating system.&lt;/p&gt; &lt;h2&gt;Install the JBoss Web Server Ansible collection&lt;/h2&gt; &lt;p&gt;Once you have RHEL 8 set up and Ansible ready to go, you need to install the &lt;a href="https://galaxy.ansible.com/middleware_automation/jws"&gt;JBoss Web Server Ansible collection&lt;/a&gt;. Ansible uses the collection to perform the following tasks on JBoss Web Server:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Ensure required dependencies are installed (e.g., unzip).&lt;/li&gt; &lt;li&gt;Install Java (if missing and requested).&lt;/li&gt; &lt;li&gt;Install the binaries and integrate the software into the system (user, group, etc.).&lt;/li&gt; &lt;li&gt;Deploy the configuration files.&lt;/li&gt; &lt;li&gt;Start and enable JBoss Web Server as a systemd service.&lt;/li&gt; &lt;li&gt;Configure a Password Vault with JBoss Web Server and a load balancer (e.g., &lt;code&gt;mod_cluster&lt;/code&gt;)—not used for this article.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Here's the installation:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ ansible-galaxy collection install middleware_automation.jws Process install dependency map Starting collection install process Installing 'middleware_automation.jws:0.0.1' to '/root/.ansible/collections/ansible_collections/middleware_automation/jws' Installing 'middleware_automation.redhat_csp_download:1.1.2' to '/root/.ansible/collections/ansible_collections/middleware_automation/redhat_csp_download'&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;a href="https://galaxy.ansible.com/"&gt;Ansible Galaxy&lt;/a&gt; fetches and downloads the collection's dependencies. That is why, at the end of the execution, the JBoss Web Server collection was installed, as well as &lt;code&gt;redhat_csp_download&lt;/code&gt;, which will help facilitate the retrieval of the archive containing the JBoss Web Server server.&lt;/p&gt; &lt;p&gt;Installing the collection reduces the configuration to achieve our automation to the bare minimum.&lt;/p&gt; &lt;h2&gt;Create a playbook to test the installation&lt;/h2&gt; &lt;p&gt;Before we continue, let's create a minimal playbook to confirm that the collection was properly installed:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - name: "JBoss Web Server installation and configuration"   hosts: "all"   become: yes   collections: – middleware_automation.jws   tasks :&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In its current state, this playbook doesn’t perform any tasks on the target system. If the playbook runs successfully, then we know that the collection has been properly installed.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# ansible-playbook -i hosts min.yml PLAY [JBoss Web Server installation and configuration] ************************************************************************* TASK [Gathering Facts] ***************************************************************************************************************** ok: [localhost] PLAY RECAP ***************************************************************************************************************************** localhost              : ok=1 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Install the Apache Tomcat web server&lt;/h2&gt; &lt;p&gt;Next, we'll install the Apache Tomcat web server, which consists of several steps.&lt;/p&gt; &lt;h3&gt;Download the archive&lt;/h3&gt; &lt;p&gt;First, let's modify our minimal playbook to retrieve the archive containing Apache Tomcat. For this purpose, we will leverage the &lt;code&gt;get_url&lt;/code&gt; module from Ansible:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - name: "JBoss Web Server installation and configuration"   hosts: "all"   become: yes   vars: tomcat_version: 9.0.50 tomcat_download_url: https://archive.apache.org/dist/tomcat/tomcat-9/v{{tomcat_version}}/bin/apache-tomcat-{{tomcat_version}}.zip tomcat_install_dir: /opt tomcat_zipfile: “{{tomcat_install_dir}}/tomcat.zip"   collections: - middleware_automation.jws pre_tasks: - name: "Download latest JBoss Web Server Zipfile from {{ tomcat_download_url }}."    get_url:      url: "{{ tomcat_download_url }}"      dest: "{{ tomcat_zipfile }}"             remote_src: yes    when:      - tomcat_download_url is defined&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The playbook downloads the archive during the &lt;code&gt;pre_tasks&lt;/code&gt; section. It’s important to use the role provided by the JBoss Web Server collection in the next step. This role will be executed before the &lt;code&gt;tasks&lt;/code&gt; block and after the &lt;code&gt;pre_tasks&lt;/code&gt; block, and it requires that the archive file be already present on the target system.&lt;/p&gt; &lt;p&gt;Execute a new playbook:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# ansible-playbook -i hosts playbook.yml PLAY [JBoss Web Server installation and configuration] ************************************************************************* TASK [Gathering Facts] ***************************************************************************************************************** ok: [localhost] TASK [Download latest JBoss Web Server Zipfile from https://archive.apache.org/dist/tomcat/tomcat-9/v9.0.50/bin/apache-tomcat-9.0.50.zip.] *** changed: [localhost] PLAY RECAP ***************************************************************************************************************************** localhost              : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Install Java&lt;/h3&gt; &lt;p&gt;JBoss Web Server is a Java-based server, so the target system is required to install a Java Virtual Machine (JVM). While Ansible primitives can perform such tasks natively, the &lt;code&gt;jws&lt;/code&gt; role can also take care of this part, provided the &lt;code&gt;tomcat_java_version&lt;/code&gt; variable is defined:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;- name: "JBoss Web Server installation and configuration"   hosts: "all"   become: yes   vars: ... tomcat_java_version: 1.8.0   collections: – middleware_automation.jws …&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Keep in mind that this feature has limits; it works only if the target system’s distribution belongs to the Red Hat &lt;a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_conditionals.html#ansible-facts-os-family"&gt;family&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ ansible -m setup localhost | grep family      "ansible_os_family": "RedHat",&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Install Java web server&lt;/h3&gt; &lt;p&gt;For Java web server to work, we need to provide one more variable to our playbook. The &lt;code&gt;tomcat_setup&lt;/code&gt; (set to &lt;code&gt;true&lt;/code&gt;) signals to the &lt;code&gt;jws&lt;/code&gt; role that we want it to perform the installation:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - name: "JBoss Web Server installation and configuration"   hosts: "all"   become: yes   vars: tomcat_setup: true ...   collections: - middleware_automation.jws   roles: - jws   pre_tasks: ...   tasks:&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Run the playbook again&lt;/h2&gt; &lt;p&gt;Let’s &lt;a href="https://gist.github.com/rpelisse/7e97b53c080c5a14ea601cf6ba5cc8bc"&gt;run our playbook again&lt;/a&gt; to see if it works as expected.&lt;/p&gt; &lt;p&gt;As you can see, quite a lot happened during this execution. Indeed, the &lt;code&gt;jws&lt;/code&gt; role took care of all the setup:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Deploying a base configuration.&lt;/li&gt; &lt;li&gt;Removing unused applications.&lt;/li&gt; &lt;li&gt;Starting the web server.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The playbooks also perform a few tasks on the target system, like installing any required dependencies if they are missing and ensuring the environment is properly configured.&lt;/p&gt; &lt;h2&gt;Configure JBoss Web Server as a systemd service&lt;/h2&gt; &lt;p&gt;A nice feature of the &lt;code&gt;jws&lt;/code&gt; role is the included functionality to configure JBoss Web Server as a systemd service. For this to occur, you just need to define the &lt;code&gt;tomcat_service_name&lt;/code&gt; variable:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - name: "JBoss Web Server installation and configuration"   hosts: "all"   become: yes   vars: … tomcat_service_name: tomcat ...   collections: - middleware_automation.jws   roles:      …&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Keep in mind this only works when systemd is installed and the system belongs to the Red Hat family.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# systemctl status tomcat ● tomcat.service - JBoss Web Server Web Application Container   Loaded: loaded (/usr/lib/systemd/system/tomcat.service; enabled; vendor preset: disabled)   Active: active (running) since Thu 2021-07-22 16:52:08 UTC; 16h ago    Main PID: 6234 (java)    Tasks: 37 (limit: 307)   Memory: 187.4M      CPU: 21.528s   CGroup: /system.slice/tomcat.service └─6234 /usr/bin/java -Djava.util.logging.config.file=/opt/apache-tomcat-9.0.50/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -classpath /opt/apache-tomcat-9.0.50/bin/bootstrap.jar:/opt/apache-tomcat-9.0.50/bin/tomcat-juli.jar -Dcatalina.base=/opt/apache-tomcat-9.0.50 -Dcatalina.home=/opt/apache-tomcat-9.0.50 -Djava.io.tmpdir=/opt/apache-tomcat-9.0.50/temp org.apache.catalina.startup.Bootstrap start Jul 22 16:52:08 4414af93c931 systemd[1]: Started Apache Tomcat Web Application Container. Jul 22 16:52:08 4414af93c931 systemd-service.sh[6220]: Tomcat started. Jul 22 16:52:08 4414af93c931 systemd-service.sh[6219]: Tomcat runs with PID: 6234&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Deploy a web application&lt;/h2&gt; &lt;p&gt;Now that JBoss Web Server is running, let’s modify the playbook and facilitate the deployment of a web application:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;tasks:     - name: " Checks that server is running"       uri:         url: "http://localhost:8080/"         status_code: 404         return_content: no     - name: "Deploy demo webapp"       get_url:         url: 'https://people.redhat.com/~rpelisse/info-1.0.war'         dest: "{{ tomcat_home }}/webapps/info.war"       notify:         - Restart Tomcat service&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A &lt;a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_handlers.html"&gt;handler&lt;/a&gt; in the &lt;code&gt;jws&lt;/code&gt; role restarts JBoss Web Server when the web application is downloaded. To finish our demonstration, we can add a quick test in the &lt;code&gt;post_tasks&lt;/code&gt; section of the playbook to confirm that the web application is functional:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;  post_tasks: - name: "Sleep for {{ tomcat_sleep }} seconds to let Tomcat starts "    wait_for:      timeout: "{{ tomcat_sleep }}" - name: "Test application"    get_url:      url: "http://localhost:8080/info/"      dest: /tmp/info.txt&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;That’s all for today! Future articles will demonstrate other features provided by the Red Hat JBoss Web Server collection, including support for &lt;code&gt;mod_cluster&lt;/code&gt; and securing the server with Tomcat’s Vault feature.&lt;/p&gt; &lt;p&gt;In the meantime, you can find the playbook used for this article in the &lt;a href="https://github.com/ansible-middleware/jws-ansible-playbook/blob/trunk/playbook.yml"&gt;Ansible Collection for JBoss Web Server GitHub repository&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/30/automate-red-hat-jboss-web-server-deployments-ansible" title="Automate Red Hat JBoss Web Server deployments with Ansible"&gt;Automate Red Hat JBoss Web Server deployments with Ansible&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/lYvQvwWxVTc" height="1" width="1" alt=""/&gt;</summary><dc:creator>Romain Pelisse</dc:creator><dc:date>2021-08-30T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/30/automate-red-hat-jboss-web-server-deployments-ansible</feedburner:origLink></entry><entry><title type="html">Emitting Process events to Kafka for Analytics using Red Hat Process Automation Manager</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/FLvqxonrVD8/emitting-process-events-to-kafka-for-analytics-using-red-hat-process-automation-manager.html" /><author><name>Sadhana Nandakumar</name></author><id>https://blog.kie.org/2021/08/emitting-process-events-to-kafka-for-analytics-using-red-hat-process-automation-manager.html</id><updated>2021-08-27T20:22:17Z</updated><content type="html">Red Hat AMQ Streams, based on Apache Kafka, is a streaming platform. Since the 7.10 release of Red Hat Process Automation Manager, users have access to out-of-box Kafka integration. Easily integrate the business processes with Kafka, for emitting or consuming events. Earlier this month, I wrote an o.n how to create a business process using Red Hat Process Automation Manager and Red Hat AMQ Streams on OpenShift. In this article, we will see how you can configure the KIE Server to emit Kafka messages about every event when a process, case, or task is completed. The KIE Server sends the messages when it commits transactions. These events can then be pushed to an analytics engine for visualizing the process metrics. Step 1: Deploy the AMQ Streams operator The Operator Hub is a collection of Operators from the Kubernetes community and Red Hat partners, curated by Red Hat. Let us first create a install the AMQ Streams operator. We will install it in a namespace we have created (rhpam-monitoring in the example). Now that the operator is installed, we can now create a simple 3 node Kafka cluster. For this click on the Kafka tab and click on the Create Kafka button. We will accept the defaults and create. Step 2: Setup Business Automation Operator Next we will first setup the business automation operator. Search for the Business Automation Operator from the operator hub. The operator allows you to create a Red Hat Process Automation manager environment with the authoring and deployment capabilities. Once the operator is deployed, switch over to the KieApp tab and click on Create KieApp. In this wizard, click on the Objects section and open up Servers. Here we will create a Kieserver definition with the environment properties for emitting the process server events. Let us now define the Bootstrap Servers as my-cluster-kafka-brokers:9092. By default, the KIE Server publishes the messages in the following topics: * jbpm-processes-events * jbpm-tasks-events * jbpm-cases-events It is possible to modify these topic names. Let us define the Processes Topic Name as rhpam-processes and the Tasks Topic Name as rhpam-tasks. For a complete list of the properties and its configuration, check out the documentation . This configuration is enough to push Process metrics to Kafka. You do not need to change anything in the process design. The configuration can be deployed using a definition as well. Step 3: Deploy a business process and create an instance Let us now open Business Central. For this lookup the route definition from the namespace. Login in to Business Central with the username/password as defined in the Business Central configuration. Import the by clicking on the Import Project. We will build and deploy the changes. Now we will create a process instance, with the following values. You can see that the process instance is created. Step 4: Push Kafka messages to a analytics tool Now that we have configured the Kafka broker, the process metrics should have been emitted to the corresponding topics. To verify this, let us deploy an open source Kafka UI ( Kafdrop) to check out the events. For deploying this, we can use the following command. oc apply -f https://raw.githubusercontent.com/snandakumar87/transaction-monitoring-dmn-kogito/master/ocp-deploy/kafdrop.yml The yaml definition for installing Kafdrop can be found . Once the deployment is completed, we can access the route definition to open up the kafka UI. You should now see the two topics created for Process and Task events. Notice how the messages in these topics are in the format. Once the metrics data is available, it is possible to push to have multiple downstream consumers reading from these topics. Let us push these metrics to an Elastic cluster. For this let us deploy the Elastic operator on Openshift.  We will create an ElasticSearchCluster and an instance of Kibana. Let us accept the defaults to complete the setup.  Now that the elastic instance is deployed, we will use a simple class to read data from the kafka topic and push it to elastic. We have defined two routes, to push the metrics to the elastic cluster.  from("kafka:" + "rhpam-processes" + "?brokers=" + kafkaBootstrap + "&amp;amp;maxPollRecords=" + consumerMaxPollRecords + "&amp;amp;seekTo=" + "beginning" + "&amp;amp;groupId=" + "process") .setHeader(Exchange.HTTP_METHOD, constant("POST")) .setHeader("Authorization",constant("Basic XXXXXXX")) .setHeader("Content-Type",constant("application/json")) .to("https://elasticsearch-sample-es-http:9200/process/process") .log("${body}"); from("kafka:" + "rhpam-tasks" + "?brokers=" + kafkaBootstrap + "&amp;amp;maxPollRecords=" + consumerMaxPollRecords + "&amp;amp;seekTo=" + "beginning" + "&amp;amp;groupId=" + "task") .setHeader(Exchange.HTTP_METHOD, constant("POST")) .setHeader("Authorization",constant("Basic XXXXXX")) .setHeader("Content-Type",constant("application/json")) .to("https://elasticsearch-sample-es-http:9200/tasks/tasks") .log("${body}"); We can now start visualizing the process/task metrics on Kibana. With this we have a whole set up within OpenShift for event-driven business applications that are focused on business process automation. The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/FLvqxonrVD8" height="1" width="1" alt=""/&gt;</content><dc:creator>Sadhana Nandakumar</dc:creator><feedburner:origLink>https://blog.kie.org/2021/08/emitting-process-events-to-kafka-for-analytics-using-red-hat-process-automation-manager.html</feedburner:origLink></entry><entry><title>Using virtual functions with DPDK on Red Hat OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Mp6zmi4tA4Y/using-virtual-functions-dpdk-red-hat-openshift" /><author><name>Wuxin Zeng</name></author><id>488c7bf9-6821-48b1-a11f-47b59d539f4b</id><updated>2021-08-27T07:00:00Z</updated><published>2021-08-27T07:00:00Z</published><summary type="html">&lt;p&gt;For many years, organizations have optimized their networking hardware by running multiple functions and containers on &lt;a href="https://docs.openshift.com/container-platform/4.3/networking/hardware_networks/about-sriov.html"&gt;Single Root I/O Virtualization&lt;/a&gt; (SR-IOV) network devices. The SR-IOV specification assigns a portion of a network interface card (NIC) or another device to a &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; pod, so that you can share the same physical NIC among multiple pods while giving the pods direct access to the network. Organizations also use the &lt;a href="https://www.dpdk.org/"&gt;Data Plane Development Kit&lt;/a&gt; (DPDK) to accelerate network traffic. This article shows you how to set up SR-IOV and DPDK on &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; and run virtual functions in that environment.&lt;/p&gt; &lt;h2&gt;Configuring SR-IOV on OpenShift&lt;/h2&gt; &lt;p&gt;You can set up SR-IOV either by editing configuration files or by using the OpenShift web console, as shown in the sections that follow. In either case, you have to create a namespace, an OperatorGroup, and a Subscription.&lt;/p&gt; &lt;h3&gt;Editing the configuration files&lt;/h3&gt; &lt;p&gt;First, create a namespace for the SR-IOV Operator. In this example, we give our namespace the name &lt;code&gt;openshift-sriov-network-operator&lt;/code&gt; and assign it a run level of 1 through the &lt;code&gt;openshift.io/run-level&lt;/code&gt; label:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: v1 kind: Namespace metadata: name: openshift-sriov-network-operator labels: openshift.io/run-level: "1" &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, create the OperatorGroup and bind it to the namespace we've just created:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: sriov-network-operators namespace: openshift-sriov-network-operator spec: targetNamespaces: - openshift-sriov-network-operator &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then, create the Subscription on the SR-IOV Operator:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: operators.coreos.com/vialpha1 kind: Subscription metadata: name: openshift-sriov-network-operator-subscription namespace: openshift-sriov-network-operator spec: channel: "4.4" name: sriov-network-operator source: redhat-operators sourceNamespace: openshift-marketplace &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Using the web console&lt;/h3&gt; &lt;p&gt;Instead of editing configuration files directly, you can do the configuration through the OpenShift console.&lt;/p&gt; &lt;p&gt;The demonstration in Figure 1 shows how to create a namespace object. If you use the &lt;strong&gt;Create Project&lt;/strong&gt; button to create the namespace, you will not be able to name it &lt;code&gt;openshift-sriov-network-operator&lt;/code&gt; because OpenShift does not allow you to create projects with names starting with &lt;code&gt;openshift-&lt;/code&gt;. You can work around the limitation by creating a namespace object.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-embedded"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;div class="field__item"&gt; &lt;img src="https://developers.redhat.com/sites/default/files/createNamespace.gif" width="1336" height="848" alt="How to use the OpenShift web console to create a namespace object." typeof="Image" /&gt;&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Creating a namespace object using the OpenShift web console.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Then, as shown in Figure 2, you can go to the OperatorHub to install the SR-IOV Network Operator, which creates both the OperatorGroup and the Subscription.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/OperatorGroup.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/OperatorGroup.gif" width="1215" height="784" alt="How to use the OpenShift web console to create the OperatorGroup and Subscription." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Creating the OperatorGroup and Subscription using the web console.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;The Node Feature Discovery Operator&lt;/h2&gt; &lt;p&gt;The SR-IOV Network Node Policy requires the following node selector in the configuration file:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;feature.node.kubernetes.io/network-sriov.capable: "true"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can install the Node Feature Discovery (&lt;code&gt;nfd&lt;/code&gt;) Operator to automatically label nodes so that you don’t need to add them manually. The Node Feature Discovery Operator allows you to find and label resources in all nodes and to include the node selector that will be required in configuring the network node policy. This operator finds nodes that are ready to support workloads with SR-IOV ports. Use the following YAML to install the operator:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: nfd namespace: openshift-operators spec: channel: "4.4" name: nfd source: redhat-operators sourceNamespace: openshift-marketplace&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, use the Node Feature Discovery Operator to create a custom resource definition (CRD) that will be managed by the operator. An example of the node feature discovery CRD follows:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: nfd.openshift.io/v1alpha1 kind: NodeFeatureDiscovery metadata: name: nfd-master-server namespace: openshift-operators spec: namespace: openshift-nfd&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can check that the operator is working by reviewing the node labels as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get node --show-labels &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can also use the console to validate the node labels, as shown in Figure 3. In the console, select &lt;strong&gt;Compute—&gt;Nodes&lt;/strong&gt;, select the node, and scroll down to see the list of node labels.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/nodelabels.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/nodelabels.gif" width="1245" height="789" alt="How to use the OpenShift web console to validate node labels." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Using the OpenShift web console to validate node labels.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Configuring NICs and virtual functions&lt;/h2&gt; &lt;p&gt;The next configuration task is to configure the NICs that will be allocable to provide SR-IOV ports. You need to specify the physical NIC as well as the number of virtual functions that can be used per NIC.&lt;/p&gt; &lt;p&gt;Not all NICs have the same number of virtual functions, so you need first to check the maximum number of virtual functions for each of your NICs. Use the Network Node State CRD in the SR-IOV Network Operator to provide the information. Enter the following command to run the operator:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc get sriovnetworknodestate -n openshift-sriov-network-operator &lt;node name&gt; -o yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Only the SR-IOV capable NICs have virtual functions. Other NICs will show none.&lt;/p&gt; &lt;p&gt;Alternatively, you can review the SR-IOV capable NICs in the web console, as shown in Figure 4. Go to &lt;strong&gt;Installed Operators&lt;/strong&gt;, select the &lt;strong&gt;Sriov&lt;/strong&gt; network state, click on a &lt;strong&gt;Sriov&lt;/strong&gt; network node, and click on &lt;strong&gt;YAML&lt;/strong&gt; to see the details.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/viewNICs.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/viewNICs.gif" width="1223" height="789" alt="How to review the SR-IOV capable NICs in the OpenShift web console." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4: Reviewing SR-IOV capable NICs in the OpenShift web console.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Now select the SR-IOV capable NICs that you want to use in the environment. For this selection, you can use the SRIOV network node policy custom resource definition created by the SR-IOV Network Operator. You can include the name of the NIC physical function and the range of virtual functions to be used in the &lt;code&gt;nicSelector&lt;/code&gt; specification, in the following format:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;&lt;pfname&gt;#&lt;first_vf&gt;-&lt;last_vf&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can change the driver type for the virtual functions in the &lt;code&gt;deviceType&lt;/code&gt; specification, which allows a choice of &lt;code&gt;netdevice&lt;/code&gt; or &lt;code&gt;vfio-pci&lt;/code&gt; for each virtual function. The &lt;code&gt;netdevice&lt;/code&gt; option performs the device binding in kernel space, whereas &lt;code&gt;vfio-pci&lt;/code&gt; does the binding in user space. The default value is &lt;code&gt;netdevice&lt;/code&gt;. However, because we’re using DPDK in this example, we will be working in user space, so we'll select &lt;code&gt;vfio-pci&lt;/code&gt; for the device type to perform the binding in user space.&lt;/p&gt; &lt;p&gt;Figure 5 shows how to select the device type using the console. Select the SR-IOV Network Operator, and create an SR-IOV network node policy instance.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/selectDeviceType.gif"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/selectDeviceType.gif" width="1245" height="789" alt="How to use the OpenShift web console to select the device type." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Selecting the device type in the web console.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The following YAML file shows that the NIC selection is done per the SR-IOV network node policy:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policy-pcie namespace: openshift-sriov-network-operator spec: resourceName: pcieSRIOV nodeSelector: feature.node.kubernetes.io/network-sriov.capable: "true" mtu: 1500 numVfs: 64 nicSelector: pfNames: ["ens1f0#0-49", "ens1f1#0-49"] deviceType: netdevice isRdma: false&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Configuring the network&lt;/h2&gt; &lt;p&gt;Next, you need to configure the network that will be attached to your SR-IOV ports in order to have SR-IOV working in your environment. When you configure the network, you can customize some aspects. If you don’t have a DHCP server in your network, you can define static IP addresses on your pods and enable static IP capability. Also, make sure the namespace is the one where you’ll be using the SR-IOV network:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetwork metadata: name: sriovnet1 namespace: openshift-sriov-network-operator spec: ipam: | { "type": "static", "addresses": [ { "address": "192.168.99.0/24", "gateway": "192.168.99.1" } ], "dns": { "nameservers": ["8.8.8.8"], "domain": "test.lablocal", "search": ["test.lablocal"] } } vlan: 0 spoofChk: 'on' trust: 'off' resourceName: onboardSRIOV networkNamespace: test-epa capabilities: '{"ips": true}'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With these configuration changes, you should have SR-IOV working in your environment. To test your configuration, create two pods and add a secondary network using the SR-IOV network. You will also need to configure static IP addresses in both pods. Add &lt;code&gt;nodeName&lt;/code&gt; definitions to force the pods to run in different worker nodes. Once the pods are running, just ping one pod from the other.&lt;/p&gt; &lt;h2&gt;Configuring DPDK on OpenShift&lt;/h2&gt; &lt;p&gt;Configuring DPDK follows the same steps for configuring SR-IOV, with a few modifications. First, DPDK requires configuring huge pages along with the SR-IOV configuration. And, as mentioned earlier, when using DPDK, you need to select a device type of &lt;code&gt;vfio-pci&lt;/code&gt; to bind the device in user space.&lt;/p&gt; &lt;p&gt;An example for an Intel NIC follows:&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you’re using a Mellanox NIC, you must use the &lt;code&gt;netDevice&lt;/code&gt; driver type and set &lt;code&gt;isRdma&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetworkNodePolicy metadata: name: policy-onboard-dpdk namespace: openshift-sriov-network-operator spec: resourceName: onboardDPDK nodeSelector: feature.node.kubernetes.io/network-sriov.capable: "true" mtu: 1500 numVfs: 64 nicSelector: pfNames: ["eno5#50-59", "eno6#50-59"] isRdma: false deviceType: vfio-pci&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The SR-IOV network object configuration for DPDK is the same as the one used for a "plain" SR-IOV configuration:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;apiVersion: sriovnetwork.openshift.io/v1 kind: SriovNetwork metadata: name: dpdknet1 namespace: openshift-sriov-network-operator spec: ipam: | { "type": "static", "addresses": [ { "address": "192.168.155.0/24", "gateway": "192.168.155.1" } ], "dns": { "nameservers": ["8.8.8.8"], "domain": "testdpdk.lablocal", "search": ["testdpdk.lablocal"] } } vlan: 0 spoofChk: 'on' trust: 'off' resourceName: onboardDPDK networkNamespace: test-epa capabilities: '{"ips": true}'&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Lastly, to test DPDK, use an image that uses DPDK to create a pod that includes both the huge pages configuration and the SR-IOV network definition.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;In this article, I have tried to show that the popular and powerful SR-IOV and DPDK capabilities of network devices are easy to set up on OpenShift. The web console simplifies many tasks, and using operators automates a lot of the configuration. Try these features on your own projects to save resources and increase the capabilities of your network hardware.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/27/using-virtual-functions-dpdk-red-hat-openshift" title="Using virtual functions with DPDK on Red Hat OpenShift"&gt;Using virtual functions with DPDK on Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Mp6zmi4tA4Y" height="1" width="1" alt=""/&gt;</summary><dc:creator>Wuxin Zeng</dc:creator><dc:date>2021-08-27T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/27/using-virtual-functions-dpdk-red-hat-openshift</feedburner:origLink></entry><entry><title type="html">This Week in JBoss - 27 August 2021</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/J-DgehC05Y8/weekly-2021-08-27.html" /><category term="quarkus" /><category term="wildfly" /><category term="keycloak" /><category term="kogito" /><category term="infinispan" /><category term="vert.x" /><category term="java" /><category term="narayana" /><author><name>Don Naro</name><uri>https://www.jboss.org/people/don-naro</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2021-08-27.html</id><updated>2021-08-27T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, wildfly, keycloak, kogito, infinispan, vert.x, java, narayana"&gt; &lt;h1&gt;This Week in JBoss - 27 August 2021&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Hello! Welcome to another edition of the JBoss Editorial that brings you news and updates from our community.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_calling_all_kogito_dmn_contributors"&gt;Calling all Kogito DMN contributors&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Guilherme Carreiro brings us the second in a series of posts that guide you through contributing to Kogito’s DMN editor, &lt;a href="https://blog.kie.org/2021/08/dmn-editor-contributors-guide-part-2.html"&gt;DMN editor – Contributors Guide – Part 2&lt;/a&gt;. The post gives a nice overview of the DMN and the BPMN editors then links to some additional resources that guide you through the tasks of creating and testing Reactive components.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_resteasy_reactive_in_quarkus_2_2"&gt;RESTEasy Reactive in Quarkus 2.2&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Clément gives a very insightful look at how Quarkus and RESTEasy Reactive make your life easier when implementing HTTP APIs. He also explains different dispatching strategies in Quarkus 2.2, based on method signatures to intelligently decide whether methods should be called on the I/O thread or a worker thread at build time.&lt;/p&gt; &lt;p&gt;Go read the full details in Clément’s post, &lt;a href="https://quarkus.io/blog/resteasy-reactive-smart-dispatch/"&gt;RESTEasy Reactive - To block or not to block&lt;/a&gt; and take a look at some of the code examples he provides.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_narayana_lra_annotation_checker"&gt;Narayana LRA annotation checker&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;The Narayana team have provided a Maven plugin that checks LRA (Long Running Actions) annotations. This plugin helps developers avoid errors and follow the rules of the LRA specification. There’s also a sample Quarkus project in that shows you how to use it.&lt;/p&gt; &lt;p&gt;You can read Ondra Chaloupka’s article here: &lt;a href="https://jbossts.blogspot.com/2021/08/lra-annotation-checker-maven-plugin.html"&gt;LRA annotation checker Maven plugin&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_devconf_us_2021_designing_your_best_architecture_diagrams_workshop"&gt;DevConf.US 2021 - Designing your best architecture diagrams workshop&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Over on Eric Schabell’s blog, he announces the &lt;a href="https://www.schabell.org/2021/07/devconfus-2021-designing-your-best-architecture-diagrams-workshop.html"&gt;Designing your best architecture diagrams&lt;/a&gt; workshop that he will deliver at this year’s DevConf.US on Thursday, September 2. Eric’s workshop takes attendees through the process of using open-source tooling to design architecture diagrams like an expert.&lt;/p&gt; &lt;p&gt;Congrats to Eric on getting the workshop accepted. I’m sure it will be an invaluable workshop for all those who face the challenges of communicating complex architectural designs to project teams.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_apache_camel_at_kubecon_eu"&gt;Apache Camel at Kubecon EU&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Nicola Ferraro presented Apache Camel at the Kubecon EU conference that took place on August 19th, 2020.&lt;/p&gt; &lt;p&gt;Nicola’s presentation focused on Camel K integration with Knative with a detailed explanation and a super cool demo. Visit his blog to read details and watch the video at &lt;a href="https://www.nicolaferraro.me/2020/09/08/serverless-integration-kubecon/"&gt;Serverless Integration on Kubernetes with Apache Camel K&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_release_roundup"&gt;Release roundup&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Let’s start things off this week with a look the latest releases.&lt;/p&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/eclipse-vertx-4-2-Beta1-released/"&gt;Eclipse Vert.x 4.2.0.Beta1&lt;/a&gt;! Great to see the first beta is here!&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-1-4-final-released/"&gt;Quarkus 2.1.4.Final&lt;/a&gt; is out! Follow the link to find the 2.1 migration guide and complete list of changes.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2021/08/kogito-1-10-0-released.html"&gt;Kogito 1.10.0&lt;/a&gt; is released alongside &lt;a href="https://blog.kie.org/2021/08/kogito-tooling-0-12-0-released.html"&gt;Kogito Tooling 0.12.0&lt;/a&gt; and the &lt;a href="https://github.com/kiegroup/kogito-operator/releases/tag/v1.10.0"&gt;Kogito Operator and CLI Version 1.10.0&lt;/a&gt;. Congrats to the entire Kogito team on all the hard work!&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.keycloak.org/2021/08/keycloak-1502-released"&gt;Keycloak 15.0.2&lt;/a&gt; is here. Grab the download link and docs from the link.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/don-naro.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Don Naro&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/J-DgehC05Y8" height="1" width="1" alt=""/&gt;</content><dc:creator>Don Naro</dc:creator><feedburner:origLink>https://www.jboss.org/posts/weekly-2021-08-27.html</feedburner:origLink></entry><entry><title>Introduction to the Node.js reference architecture, Part 5: Building good containers</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/BAjbWYmGu_w/introduction-nodejs-reference-architecture-part-5-building-good-containers" /><author><name>Michael Dawson</name></author><id>d329e613-6428-42d6-93de-40df642014a6</id><updated>2021-08-26T07:00:00Z</updated><published>2021-08-26T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/containers/"&gt;Containers&lt;/a&gt; are often the unit of deployment in modern applications. An application is built into one or more container images using Docker or Podman, and then those images are deployed into production.&lt;/p&gt; &lt;p&gt;A container packages code (in our case, written in &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt;) along with its dependencies so that they can be easily deployed as a unit. The &lt;a href="https://opencontainers.org/"&gt;Open Container Initiative&lt;/a&gt; (OCI) defines the standard for what makes up a container.&lt;/p&gt; &lt;p&gt;This article dives into the discussions that went into creating the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/building-good-containers.md"&gt;Building Good Containers&lt;/a&gt; section of the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;Node.js reference architecture&lt;/a&gt;. That section focuses on how the container is built, versus how to structure an application for deployment in a container. Other sections in the reference architecture, like &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/operations/healthchecks.md"&gt;Health Checks&lt;/a&gt; and &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/operations/logging.md"&gt;Logging&lt;/a&gt;, cover how to structure an application for cloud-native deployments.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Read the series so far&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt;&lt;li class="Indent1"&gt;Part 1: &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview" target="_blank"&gt;Overview of the Node.js reference architecture&lt;/a&gt;&lt;/li&gt; &lt;li class="Indent1"&gt;Part 2: &lt;a href="https://developer.ibm.com/languages/node-js/blogs/nodejs-reference-architectire-pino-for-logging/" target="_blank"&gt;Logging in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li class="Indent1"&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2021/05/17/introduction-nodejs-reference-architecture-part-3-code-consistency" target="_blank"&gt;Code consistency in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li class="Indent1"&gt;Part 4: &lt;a href="https://developers.redhat.com/articles/2021/06/22/introduction-nodejs-reference-architecture-part-4-graphql-nodejs"&gt;GraphQL in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li class="Indent1"&gt;&lt;strong&gt;Part 5&lt;/strong&gt;: Building good containers&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;What makes a good production container?&lt;/h2&gt; &lt;p&gt;Before we dive into the recommendations for building good containers, what do we mean by a "good" container in the first place? What this means to the Red Hat and IBM team members is that the container:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Applies best practices for security.&lt;/li&gt; &lt;li&gt;Is a reasonable size.&lt;/li&gt; &lt;li&gt;Avoids common pitfalls with running a process in a container.&lt;/li&gt; &lt;li&gt;Can take advantage of the resources provided to it.&lt;/li&gt; &lt;li&gt;Includes what’s needed to debug production issues when they occur.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;While the relative priority between these can differ across teams, these were generally important based on our experience.&lt;/p&gt; &lt;h2&gt;What base images to start with?&lt;/h2&gt; &lt;p&gt;In most cases, teams build their containers based on a pre-existing image that includes at least the operating system (OS) and commonly also includes the runtime—in our case, Node.js. In order to build good containers, it is important to start on solid footing by choosing a base container that is well maintained, is scanned and updated when vulnerabilities are reported, keeps up with new versions of the runtime, and (if required by your organization) has commercial support. The reference architecture includes two sections that talk about containers: &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/functional-components/nodejs-versions-images.md#container-images"&gt;Container images&lt;/a&gt; and &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/functional-components/nodejs-versions-images.md#commercially-supported-containers"&gt;Commercially Supported Containers&lt;/a&gt;. Most of the teams within Red Hat and IBM are already using or moving toward using the Node.js &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Red Hat Universal Base Images &lt;/a&gt; (UBI) for Node.js deployments.&lt;/p&gt; &lt;h2&gt;Apply security best practices&lt;/h2&gt; &lt;p&gt;The first thing we talked about with respect to building good containers is making sure we applied security best practices. The two recommendations that came from these discussions were:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Build containers so that your application runs as non-root.&lt;/li&gt; &lt;li&gt;Avoid reserved (privileged) ports (1–1023) inside the container.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The reason for building containers so that your application runs as non-root is well-documented, and we found it was a common practice across the team members. For a good article that dives into the details, see &lt;a href="https://medium.com/@mccode/processes-in-containers-should-not-run-as-root-2feae3f0df3b#:~:text=Containers%20are%20not%20trust%20boundaries,a%20container%20on%20your%20server"&gt;Processes In Containers Should Not Run As Root&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Why should you avoid using reserved (privileged) ports (1-1023)? Docker or &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt; will just map the port to something different anyway, right? The problem is that applications not running as root normally cannot bind to ports 1-1023, and while it might be possible to allow this when the container is started, you generally want to avoid it. In addition, the Node.js runtime has some limitations that mean if you add the privileges needed to run on those ports when starting the container, you can no longer do things like set additional certificates in the environment. Since the ports will be mapped anyway, there is no good reason to use a reserved (privileged) port. Avoiding them can save you trouble in the future.&lt;/p&gt; &lt;h3&gt;A real-world example: A complicated migration&lt;/h3&gt; &lt;p&gt;Using reserved (privileged) ports inside a container led to a complicated migration process for one of our teams when they later wanted to move to a new base container that was designed to run applications as non-root.&lt;/p&gt; &lt;p&gt;The team had many &lt;a href="https://developers.redhat.com/topics/microservices/"&gt;microservices&lt;/a&gt; all using the same set of internal ports, and they wanted to be able to slowly update and deploy individual microservices without having to modify the configurations outside of the container. Using different ports internally would have meant they would have to maintain the knowledge of which microservices used which ports internally, and that would make the configuration more complex and harder to maintain. The problem was that with the new base image, the microservices could no longer bind to the internal privileged port they had been using before.&lt;/p&gt; &lt;p&gt;The team thought, "Okay, so let's just use iptables or some other way to redirect so that even when the application binds to a port above 1023, Kubernetes still sees the service as exposed on the original privileged port." Unfortunately, that's not something that developers are expected to do in containers, and base containers don't include the components for port forwarding!&lt;/p&gt; &lt;p&gt;Next, they said, "Okay, let's give the containers the privileges required so that a non-root user can connect to the privileged port." Unfortunately, due to the issue in Node.js, that led to not being able to set additional certificates that they needed. In the end, the team found a way to migrate, but it was a lot more complicated than if they had not been using privileged ports.&lt;/p&gt; &lt;h2&gt;Keep containers to a reasonable size&lt;/h2&gt; &lt;p&gt;A common question is, "Why does container size matter?" The expectation is that with good layering and caching, the total size of a container won't end up being an issue. While that can often be true, environments like Kubernetes make it easy for containers to spin up and down and do so on different machines. Each time this happens on a new machine, you end up having to pull down all of the components. The same happens for new deployments if you updated all of the layers starting at the OS (maybe to address CVEs).&lt;/p&gt; &lt;p&gt;The net is that while we've not seen complaints or had problems in our deployments with respect to the size on disk, the compressed size that might need to be transferred to a machine has led our teams to strive to minimize container size.&lt;/p&gt; &lt;p&gt;A common practice we discussed was multi-stage builds, where you build in a larger base container and then copy the application artifacts to a smaller deployment image. The document &lt;a href="https://docs.docker.com/develop/develop-images/multistage-build/"&gt;Use multi-stage builds&lt;/a&gt; provides a good overview of how to do that.&lt;/p&gt; &lt;h2&gt;Support efficient iterative development&lt;/h2&gt; &lt;p&gt;The discussions on keeping container sizes reasonable also resulted in a few additional recommendations from our experience that I was unaware of before. (The process of putting together the reference architecture has been a great learning experience all around.)&lt;/p&gt; &lt;p&gt;The first was to use the &lt;code&gt;.dockerignore&lt;/code&gt; file. Once I thought about it, it made a lot of sense, as I'd run into one of the issues it addresses a number of times. If you test locally and do an &lt;code&gt;npm install&lt;/code&gt;, you end up with the &lt;code&gt;node_modules&lt;/code&gt; directory locally. When you run your Docker file, it will take longer, as it copies that directory over even though it won't necessarily be used in the build step (and if it is, that could mess things up). Assuming you are using a multi-stage build, it won't affect your final image size, but it does affect the speed of development as you iterate.&lt;/p&gt; &lt;p&gt;The second recommendation was to use a dependency image. For many applications, the build time is dominated by the time it takes to build the dependencies. If you break out your pipeline so that you build a dependency image and then layer your application into that image, the process of updating and testing the application can be much faster. This is because, for most of the iterations, you will not have updated the dependencies and can skip the slower rebuild of the dependency layer.&lt;/p&gt; &lt;h2&gt;Build containers that can take advantage of the resources provided&lt;/h2&gt; &lt;p&gt;The nice thing about using containers is that it decouples the application, microservice, etc., from the physical resources on which it will be deployed. It also means that the resources available to the container might change. Kubernetes, Docker, and Podman all provide ways to change the available resources when a container is started. If you don't plan or think about this in advance, you can end up with a container that overuses or underuses the resources available to it, resulting in poorer performance than expected.&lt;/p&gt; &lt;p&gt;In our discussions, we found that teams had developed patterns to start Node.js applications within containers such that they could leverage the amount of memory made available when the container was deployed. The &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/building-good-containers.md#setting-memory-limits"&gt;reference architecture shares this pattern&lt;/a&gt; as good practice so that your application leverages the available amount of resources. Since Node.js is "approximately" single-threaded, we had not found the need to pass through available CPU resources to the same extent.&lt;/p&gt; &lt;h2&gt;Be ready to debug production issues when they occur&lt;/h2&gt; &lt;p&gt;When things go wrong in production, you often need additional tools to help investigate what is going on. While we did not have a common set of tools to recommend from across our teams at this point, there was consensus that it is best practice to include key tools that you might need for problem investigation. This is one reason why we've been working in the Node.js project to pull some diagnostic tools into core (such as &lt;code&gt;node-report&lt;/code&gt;, the ability to generate heap dumps, and the sampling heap profiler).&lt;/p&gt; &lt;h2&gt;Avoid common pitfalls when running a process in a container&lt;/h2&gt; &lt;p&gt;Running a Node.js process in a container is different from running on a full operating system. This results in a couple of common pitfalls related to signals, child processes, and zombies, in no particular order. Our teams ran into a number of these challenges, which resulted in the recommendations to &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/building-good-containers.md#use-a-process-manager"&gt;use a process manager&lt;/a&gt; and &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/building-good-containers.md#avoiding-using-npm-to-start-application"&gt;avoid the use of &lt;code&gt;npm start&lt;/code&gt;&lt;/a&gt;. There is not much to add here (the reference architecture provides helpful resources for further reading), other than to say these are real-world issues that one or more of our teams have run into.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Building good containers can result in both faster development cycles and better deployments with fewer problems. In this article, we've shared some of the discussion and background that resulted in the recommendations in the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/building-good-containers.md"&gt;Building Good Containers&lt;/a&gt; section of the Node.js reference architecture.&lt;/p&gt; &lt;p&gt;We hope you find these recommendations useful. While you wait for the next installment in the &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview"&gt;Introduction to the Node.js reference architecture series&lt;/a&gt;, you can check out the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;GitHub project&lt;/a&gt; to explore sections that might be covered in future articles.&lt;/p&gt; &lt;p&gt;If you want to learn more about what Red Hat is up to on the Node.js front, you can also explore the &lt;a href="m/topics/nodejs"&gt;Node.js topic page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/26/introduction-nodejs-reference-architecture-part-5-building-good-containers" title="Introduction to the Node.js reference architecture, Part 5: Building good containers"&gt;Introduction to the Node.js reference architecture, Part 5: Building good containers&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/BAjbWYmGu_w" height="1" width="1" alt=""/&gt;</summary><dc:creator>Michael Dawson</dc:creator><dc:date>2021-08-26T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/26/introduction-nodejs-reference-architecture-part-5-building-good-containers</feedburner:origLink></entry><entry><title type="html">Quarkus 2.1.4.Final released - Maintenance release</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ofQfKz4Xbos/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-2-1-4-final-released/</id><updated>2021-08-26T00:00:00Z</updated><content type="html">We just released Quarkus 2.1.4.Final, our fourth (and probably last!) maintenance release on top of 2.1. It is a safe upgrade for anyone already using 2.1. If you are not using 2.1 already, please refer to the 2.1 migration guide. Full changelog You can get the full changelog of 2.1.4.Final...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ofQfKz4Xbos" height="1" width="1" alt=""/&gt;</content><dc:creator>Guillaume Smet</dc:creator><feedburner:origLink>https://quarkus.io/blog/quarkus-2-1-4-final-released/</feedburner:origLink></entry><entry><title>Securing malloc in glibc: Why malloc hooks had to go</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/gLP8-dV0DFI/securing-malloc-glibc-why-malloc-hooks-had-go" /><author><name>Siddhesh Poyarekar</name></author><id>0accb512-3e3e-4517-8638-6fdf07c07f2d</id><updated>2021-08-25T07:00:00Z</updated><published>2021-08-25T07:00:00Z</published><summary type="html">&lt;p&gt;Memory access is one of the most basic operations in computer programs. It is also an unending source of program errors in C programs, because memory safety was never really a programming language goal in C. Memory-related issues also comprise a significant part of the &lt;a href="https://cwe.mitre.org/top25/archive/2021/2021_cwe_top25.html"&gt;top 25 security weaknesses&lt;/a&gt; that result in program vulnerabilities.&lt;/p&gt; &lt;p&gt;Memory access also plays an important role in performance, which makes memory management a prime target for performance tuning. It is natural, then, that dynamic memory management in the C runtime should have capabilities that allow fine-grained tracking and customizable actions on allocation events. These features allow users to diagnose memory issues in their programs and if necessary, override the C runtime allocator with their own to improve performance or memory utilization.&lt;/p&gt; &lt;p&gt;This article describes the clash between the quest for flexibility and introspection, on the one hand, and performance and security protections on the other. You'll learn why this clash ultimately led to a major change in how memory allocation (&lt;code&gt;malloc&lt;/code&gt;) is implemented in the &lt;a href="http://www.gnu.org/software/libc/libc.html"&gt;GNU C Library&lt;/a&gt;, or glibc. We'll also discuss how to adapt applications that depended on the old way of doing things, as well as the implications for future versions of Fedora and &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL).&lt;/p&gt; &lt;h2&gt;Debugging malloc in glibc&lt;/h2&gt; &lt;p&gt;Until recently, the GNU C Library, which is the core runtime library for RHEL, provided diagnostic functionality in the form of function pointers that users could overwrite to implement their own allocation functions. These function pointers were collectively called &lt;em&gt;malloc hooks&lt;/em&gt;. If a hook was set to a function address, glibc allocator functions would call the function instead of the internal implementations, allowing programmers to perform arbitrary actions. A programmer could even run a custom function and then, if necessary, call the glibc memory allocator function again (by momentarily setting the hook to &lt;code&gt;NULL&lt;/code&gt;) to get the actual block of memory.&lt;/p&gt; &lt;p&gt;All of the malloc debugging features in glibc (i.e., &lt;code&gt;mtrace&lt;/code&gt;, &lt;code&gt;mcheck&lt;/code&gt;, and the &lt;code&gt;MALLOC_CHECK_&lt;/code&gt; environment variable) were implemented using these hooks. These debugging features, and the hooks in general, were very useful because they provided checking on a more lightweight basis than the memory checking done by full-fledged memory debugging programs such as &lt;a href="https://developers.redhat.com/blog/2021/04/23/valgrind-memcheck-different-ways-to-lose-your-memory"&gt;Valgrind&lt;/a&gt; and &lt;a href="https://developers.redhat.com/blog/2021/05/05/memory-error-checking-in-c-and-c-comparing-sanitizers-and-valgrind"&gt;sanitizers&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Malloc hooks in multi-threaded applications&lt;/h2&gt; &lt;p&gt;As applications became increasingly multi-threaded, it was discovered that manipulating malloc hooks in such environments was fraught with risks. All of the debugging features in glibc malloc except &lt;code&gt;MALLOC_CHECK_&lt;/code&gt; were, and continue to be, unsafe in multi-threaded environments. Malloc hooks, the basis of the debugging features, were not the only way to override malloc, either; glibc always supported the &lt;a href="https://www.gnu.org/software/libc/manual/html_node/Replacing-malloc.html#Replacing-malloc"&gt;interposition of malloc functions&lt;/a&gt; by preloading a shared library with those functions. Glibc itself always calls malloc functions through its procedure linkage table (PLT) so that it can invoke the interposed functions.&lt;/p&gt; &lt;p&gt;To make things worse, much of the debugging infrastructure was tightly integrated into system allocator functionality. This made the task of enhancing the allocator unnecessarily complex. Furthermore, there was always the possibility of corner cases inducing unexpected behavior. Finally, implementing debugging features in the system allocator created a minor but unnecessary performance overhead.&lt;/p&gt; &lt;p&gt;The key misfeature of the debugging hooks, though, was their presence as unprotected function pointers that were guaranteed to be executed at specific events. This made the hooks an &lt;a href="https://gist.github.com/romanking98/9aab2804832c0fb46615f025e8ffb0bc"&gt;easy exploit primitive&lt;/a&gt; in practically every program that ran on &lt;a href="https://developers.redhat.com/topics/linux"&gt;Linux&lt;/a&gt; distributions. A trivial search for &lt;a href="https://duckduckgo.com/?q=__malloc_hook+%22house+of%22"&gt;__malloc_hook "house of"&lt;/a&gt; turns up a long list of exploit methods that use the hooks as either an intermediate step or the final goal for the exploit.&lt;/p&gt; &lt;p&gt;Malloc hooks had to go.&lt;/p&gt; &lt;h2&gt;Excising malloc hooks from the main library&lt;/h2&gt; &lt;p&gt;The last of the malloc hook variables were deprecated in glibc 2.32 and new applications were encouraged to use malloc interposition instead. The effect of deprecation was mild: Newer applications just got a warning during the build. In the interest of maintaining backward compatibility, the memory allocator continued to look for hooks and, if available, execute them. In glibc version 2.34 (August 2021), we finally bit the bullet and took support for malloc hooks out of the mainstream library.&lt;/p&gt; &lt;p&gt;The upstream glibc community agreed that malloc debugging features have no place in production. So we moved all debugging features into a separate library named &lt;code&gt;libc_malloc_debug.so.0&lt;/code&gt; that overrides system malloc behavior to enable debugging. Most importantly, we either moved unprotected hook function pointers into &lt;code&gt;libc_malloc_debug.so.0&lt;/code&gt; or removed them completely. Doing this eliminated a key exploit primitive from the library.&lt;/p&gt; &lt;h2&gt;Debugging and hardening in a post-hook world&lt;/h2&gt; &lt;p&gt;The new glibc without the problematic hooks will be available in future versions of Fedora and RHEL. With this glibc, malloc debugging features such as &lt;code&gt;MALLOC_CHECK_&lt;/code&gt;, &lt;code&gt;mtrace()&lt;/code&gt;, and &lt;code&gt;mcheck()&lt;/code&gt; will no longer work by default. Users will need to preload &lt;code&gt;libc_malloc_debug.so.0&lt;/code&gt; to enable these debugging features. Additionally, the &lt;code&gt;__after_morecore_hook&lt;/code&gt;, &lt;code&gt;__default_morecore_hook&lt;/code&gt;, and &lt;code&gt;__morecore&lt;/code&gt; function pointers are no longer read, and the system malloc uses the &lt;code&gt;brk()&lt;/code&gt; and &lt;code&gt;mmap()&lt;/code&gt; system calls to request memory from the kernel.&lt;/p&gt; &lt;p&gt;System administrators may also remove the library from the system and effectively disable malloc debugging and malloc hooks. This is useful hardening for production systems that have strong controls on what files are available on the system.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Separating malloc debugging from the main library is a significant security hardening improvement in glibc. It eliminates an exploit primitive from Linux distributions and adds an opportunity for hardening in both RHEL and Fedora. Simplifying system allocator code also sets the stage for improvements to malloc that may result in better security and performance. Watch out for more interesting changes to the malloc subsystem in future releases of glibc.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/25/securing-malloc-glibc-why-malloc-hooks-had-go" title="Securing malloc in glibc: Why malloc hooks had to go"&gt;Securing malloc in glibc: Why malloc hooks had to go&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/gLP8-dV0DFI" height="1" width="1" alt=""/&gt;</summary><dc:creator>Siddhesh Poyarekar</dc:creator><dc:date>2021-08-25T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/25/securing-malloc-glibc-why-malloc-hooks-had-go</feedburner:origLink></entry></feed>
